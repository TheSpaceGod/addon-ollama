# Home Assistant Add-on: Ollama [CPU]

Ollama‚Å† makes it easy to get up and running with large language models locally.
This addon variant uses the CPU for all model inference operations.

Documentation: <https://github.com/ollama/ollama/tree/main/docs>

## Configuration

For a list of all configuration options and definitions, view this Ollama code: <https://github.com/ollama/ollama/blob/main/envconfig/config.go#L232>
